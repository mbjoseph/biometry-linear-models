---
title: "The problem with parameters: using information theory to evaluate models"
author: "Max Joseph"
date: "March 16, 2017"
output:
  beamer_presentation:
    fonttheme: "structurebold"
    fig_caption: false
    fig_width: 6
    fig_height: 4
header-includes: 
- \beamertemplatenavigationsymbolsempty
- \definecolor{foreground}{RGB}{0,0,0}
- \definecolor{background}{RGB}{255,255,255}
- \definecolor{title}{RGB}{0,120,255}
- \definecolor{gray}{RGB}{155,155,155}
- \definecolor{subtitle}{RGB}{204,0,0}
- \definecolor{hilight}{RGB}{102,255,204}
- \definecolor{vhilight}{RGB}{255,111,207}
- \setbeamercolor{titlelike}{fg=title}
- \setbeamercolor{subtitle}{fg=subtitle}
- \setbeamercolor{institute}{fg=gray}
- \setbeamercolor{normal text}{fg=foreground,bg=background}
- \setbeamercolor{local structure}{fg=title}
- \setbeamertemplate{frametitle}{\begin{centering} \insertframetitle \par \end{centering}}
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width = 4, fig.height = 2.5,
                      fig.align = 'center', echo = FALSE,
                      warning = FALSE, message = FALSE)
library(tidyverse)
library(gridExtra)
library(purrr)
theme_set(theme_minimal())
```

# $\hat{y} = \beta_0 + \beta_1 x$

```{r}
n <- 10
x <- rnorm(n, 0, 2)
y <- rnorm(n, x + .4 * x^3, 1.5)
d <- tibble(x, y)

dnew <- tibble(x = runif(100, min(x), max(x))) %>%
  mutate(y = rnorm(100, x + .4 * x^3, 1.5))

d %>%
  ggplot(aes(x, y)) + 
  geom_point() + 
  geom_smooth(size = .5, method = "lm", se = FALSE)
```

# $\hat{y} = \beta_0 + \beta_1 x + \beta_2 x^2$

```{r}
d %>%
  ggplot(aes(x, y)) + 
  geom_point() + 
  geom_smooth(size = .5, method = "lm", se = FALSE, formula = y ~ poly(x, 2))
```


# $\hat{y} = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3$

```{r}
d %>%
  ggplot(aes(x, y)) + 
  geom_point() + 
  geom_smooth(size = .5, method = "lm", se = FALSE, formula = y ~ poly(x, 3))
```


# $\hat{y} = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 x^4$

```{r}
d %>%
  ggplot(aes(x, y)) + 
  geom_point() + 
  geom_smooth(size = .5, method = "lm", se = FALSE, formula = y ~ poly(x, 4))
```

# $\hat{y} = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 x^4 + \beta_5 x^5$

```{r}
d %>%
  ggplot(aes(x, y)) + 
  geom_point() + 
  geom_smooth(size = .5, method = "lm", se = FALSE, formula = y ~ poly(x, 5))
```

# $\hat{y} = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 x^4 + \beta_5 x^5 + \beta_6 x^6$

```{r}
d %>%
  ggplot(aes(x, y)) + 
  geom_point() + 
  geom_smooth(size = .5, method = "lm", se = FALSE, formula = y ~ poly(x, 6))
```

# $\hat{y} = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 x^4 + \beta_5 x^5 + \beta_6 x^6 + \beta_7 x^7$

```{r}
d %>%
  ggplot(aes(x, y)) + 
  geom_point() + 
  geom_smooth(size = .5, method = "lm", se = FALSE, formula = y ~ poly(x, 7))
```

# $\hat{y} = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 x^4 + \beta_5 x^5 + \beta_6 x^6 + \beta_7 x^7 + \beta_8 x^8$

```{r}
d %>%
  ggplot(aes(x, y)) + 
  geom_point() + 
  geom_smooth(size = .5, method = "lm", se = FALSE, formula = y ~ poly(x, 8))
```

# $\hat{y} = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 x^4 + \beta_5 x^5 + \beta_6 x^6 + \beta_7 x^7 + \beta_8 x^8 + \beta_9 x^9$

```{r}
d %>%
  ggplot(aes(x, y)) + 
  geom_point() + 
  geom_smooth(size = .5, method = "lm", se = FALSE, formula = y ~ poly(x, 9))
```

# Overfitting

Parameters begin to fit to **noise**

- good fit to **training data**
- bad predictions for out of sample data



# $\hat{y} = \beta_0 + \beta_1 x$

```{r}
d %>%
  ggplot(aes(x, y)) + 
  geom_point(alpha = .1) + 
  geom_smooth(size = .5, method = "lm", se = FALSE) + 
  geom_point(data = dnew, color = "purple")
```

# $\hat{y} = \beta_0 + \beta_1 x + \beta_2 x^2$

```{r}
d %>%
  ggplot(aes(x, y)) + 
  geom_point(alpha = .1) + 
  geom_smooth(size = .5, method = "lm", se = FALSE, formula = y ~ poly(x, 2)) + 
  geom_point(data = dnew, color = "purple")
```


# $\hat{y} = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3$

```{r}
d %>%
  ggplot(aes(x, y)) + 
  geom_point(alpha = .1) + 
  geom_smooth(size = .5, method = "lm", se = FALSE, formula = y ~ poly(x, 3)) + 
  geom_point(data = dnew, color = "purple")
```


# $\hat{y} = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 x^4$

```{r}
d %>%
  ggplot(aes(x, y)) + 
  geom_point(alpha = .1) + 
  geom_smooth(size = .5, method = "lm", se = FALSE, formula = y ~ poly(x, 4)) + 
  geom_point(data = dnew, color = "purple")
```

# $\hat{y} = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 x^4 + \beta_5 x^5$

```{r}
d %>%
  ggplot(aes(x, y)) + 
  geom_point(alpha = .1) + 
  geom_smooth(size = .5, method = "lm", se = FALSE, formula = y ~ poly(x, 5)) + 
  geom_point(data = dnew, color = "purple")
```

# $\hat{y} = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 x^4 + \beta_5 x^5 + \beta_6 x^6$

```{r}
d %>%
  ggplot(aes(x, y)) + 
  geom_point(alpha = .1) + 
  geom_smooth(size = .5, method = "lm", se = FALSE, formula = y ~ poly(x, 6)) + 
  geom_point(data = dnew, color = "purple")
```

# $\hat{y} = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 x^4 + \beta_5 x^5 + \beta_6 x^6 + \beta_7 x^7$

```{r}
d %>%
  ggplot(aes(x, y)) + 
  geom_point(alpha = .1) + 
  geom_smooth(size = .5, method = "lm", se = FALSE, formula = y ~ poly(x, 7)) + 
  geom_point(data = dnew, color = "purple")
```

# $\hat{y} = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 x^4 + \beta_5 x^5 + \beta_6 x^6 + \beta_7 x^7 + \beta_8 x^8$

```{r}
d %>%
  ggplot(aes(x, y)) + 
  geom_point(alpha = .1) + 
  geom_smooth(size = .5, method = "lm", se = FALSE, formula = y ~ poly(x, 8)) + 
  geom_point(data = dnew, color = "purple")
```

# $\hat{y} = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 x^4 + \beta_5 x^5 + \beta_6 x^6 + \beta_7 x^7 + \beta_8 x^8 + \beta_9 x^9$

```{r}
d %>%
  ggplot(aes(x, y)) + 
  geom_point(alpha = .1) + 
  geom_smooth(size = .5, method = "lm", se = FALSE, formula = y ~ poly(x, 9)) + 
  geom_point(data = dnew, color = "purple")
```

# Underfitting

Model is too simplistic to capture signal

```{r}
d %>%
  ggplot(aes(x, y)) + 
  geom_point(alpha = .1) + 
  geom_smooth(size = .5, method = "lm", se = FALSE) + 
  geom_point(data = dnew, color = "purple")
```

# Today

Working toward **information criteria** to balance: 

- model complexity
- out of sample predictive power

# Roadmap

**Finding a way to compare models**

1. Information entropy
2. Kullback-Leibler divergence
3. Deviance
4. Akaike's information criterion

# Information entropy

Uncertainty contained in a probability distribution 

$$H(p) = - \sum_{i = 1}^{n} p_i \log(p_i)$$

- $H(p)$: information entropy of a distribution $p$
- $n$: the number of possible outcomes
- $p_i$: the probability of outcome $i$

# Activity

Compute the information entropy for your die!

$$H(p) = - \sum_{i = 1}^{n} p_i \log(p_i)$$

# What if we didn't know anything about dice?

Find an estimate of information entropy:

1. Estimate $p_1, p_2, p_3, ...$

2. Compute information entropy of your estimated distribution:

$$H(\hat{p}) = - \sum_{i = 1}^{n} \hat{p}_i \log(\hat{p}_i)$$

# Divergence

How far off is our model from the true distribution?

**Example**

We used $\hat{p}$ to estimate $p$

- What's the "divergence" between $\hat{p}$ and $p$?

# Kullback-Leibler divergence

How far off is our model $q$ from the true distribution $p$?

$$D_{\text{KL}} = \sum_{i=1}^n p_i \log \Big(\frac{p_i}{q_i}\Big)$$

# Activity

Calculate KL divergence for the following sample sizes: 

- 5
- 10
- 20

$$D_{\text{KL}} = \sum_{i=1}^n p_i (\log (p_i) - \log(q_i))$$

* Average difference in log probability between $p$ and $q$

**Bonus** 

- What happens when you draw 1000 samples?
- What happens when our approximation $q$ is exactly the same as $p$?

# The problem with reality

We almost never know the true probability of events!

# What *do* we have

Typically we have data $y_1, y_2, ..., y_n$

and some models (let's say two) 

$$q, r$$

**So we can ask**

Which model seems closer to the true distribution $p$?

$$D_{\text{KL}}(p, q) - D_{\text{KL}}(p, r) = -(E \log(q_i) - E \log(r_i))$$

# Comparing models $q$ and $r$


$$D_{\text{KL}}(p, q) - D_{\text{KL}}(p, r) = -(E \log(q_i) - E \log(r_i))$$

**Notice that we don't need $p$ to compute this difference!**

# Deviance 

$$D_{\text{KL}}(p, q) - D_{\text{KL}}(p, r) = -(E \log(q_i) - E \log(r_i))$$

We can plug in something proportional to the expected log likelihood:

$$E \log(q_i) \propto$$ 

$$D(q) = -2 \sum_{i = 1}^n log(q_i)$$

where $D(q)$ is the **Deviance**


# Deviance in plain english

$$D(q) = -2 \sum_{i = 1}^n log(q_i)$$

A relative measure of divergence from the true distribution $p$

- one deviance value is useless
- multiple values allow us to compare models


# How to calculate deviance

$$D(q) = -2 \sum_{i = 1}^n log(q_i)$$

Log likelihood: $\sum_{i = 1}^n log(q_i)$

$\rightarrow$ multiply log likelihood by -2.

*demo

# The problem with Deviance

New predictors improve (reduce) deviance

- same problem as $R^2$

```{r}
# split into training and test sets
trees$train <- rep_len(c(0, 1), nrow(trees))
train <- subset(trees, train == 1)
test <- subset(trees, train == 0)


m1 <- lm(Volume ~ Girth, data = train)
m1dev <- -2 * logLik(m1)
train %>%
  ggplot(aes(Girth, Volume)) + 
  stat_smooth(method = "lm", formula = y ~ poly(x, 1), se = FALSE) + 
  geom_point() + 
  ggtitle(paste("Deviance:", round(m1dev, 3)))
```

# The problem with Deviance

New predictors improve (reduce) deviance

- same problem as $R^2$

```{r}
# use an 8th degree polynomial
m <- lm(Volume ~ poly(Girth, 8), data = train)
mdev <- -2 * logLik(m)
train %>%
  ggplot(aes(Girth, Volume)) + 
  stat_smooth(method = "lm", formula = y ~ poly(x, 8), se = FALSE) + 
  geom_point() + 
  ggtitle(paste("Deviance:", round(mdev, 3)))
```

# At the end of the day

We want to be close to the **truth** but not too close to our training **data**

# We want to make good predictions

In other words, we'd like low deviance for **new** observations

Deviance of training data ($q_1, q_2, ...$):

$$D_{\text{train}}(q) = -2 \sum_i log(q_i)$$

Deviance of future data ($\tilde{q}_1, \tilde{q}_2, ...$):

$$D_{\text{test}}(q) = -2 \sum_i log(\tilde{q}_i)$$

# Deviance of the training and test set

```{r}
dev_plot <- function(degree) {
  m <- lm(Volume ~ poly(Girth, degree), data = train)
  train_dev <- -2 * logLik(m)
  xline <- data.frame(Girth = seq(min(trees$Girth), 
                                  max(trees$Girth), 
                                  length.out = 100))
  xline$Volume <- predict(m, xline)
  
  mu_test <- predict(m, test)
  test_ll <- sum(dnorm(train$Volume, mu_test, sigma(m), log = TRUE))
  test_dev <- -2 * test_ll
  
  p <- train %>%
    ggplot(aes(Girth, Volume)) + 
    geom_line(data = xline, color = "dodgerblue") + 
    geom_point() + 
    ggtitle(paste0("Training deviance: ", round(train_dev, 3), ";    ",
                  "Test deviance: ", round(test_dev, 3))) + 
    geom_point(data = test, color = "purple") + 
    theme(plot.title = element_text(size = rel(.8)))
  list(p, dev_df = tibble(Deviance = c(train_dev, test_dev), 
                          Sample = c("Training set", "Test set")))
}

dev_plot(1)[[1]]
# Check training deviance values
```

# Deviance of the training and test set

```{r}
dev_plot(2)[[1]]
```


# Deviance of the training and test set

```{r}
dev_plot(3)[[1]]
```

# Deviance of the training and test set

```{r}
dev_plot(4)[[1]]
```

# Deviance of the training and test set

```{r}
dev_plot(5)[[1]]
```


# Training and test deviance across a range of model complexity

```{r}
l <- list()
for (i in 1:5) {
  l[[i]] <- dev_plot(i)[[2]]
}

bind_rows(l, .id = "Polynomial degree") %>%
  ggplot(aes(`Polynomial degree`, Deviance, color = Sample)) + 
  geom_point() + 
  scale_y_log10() + 
  scale_color_manual(values = c("red", "blue"), "")
```

# Some problems with training vs. test splits

1. How to decide what goes where?
2. What if you have a small dataset?
3. What if your data are structured (e.g., by spatial location)

# Enter AIC

Instead of computing $D_{\text{test}}$, approximate with 

## Akaike's information criterion

$$AIC = D_{\text{train}} + 2p$$
  
- $D_{\text{train}}$ is your training set deviance
- $p$ is the number of parameters in your model

"Better" models have lower $AIC$
  
# Recap:

1. Over vs. underfitting
2. Measured how close a model $q$ is to truth $p$ (KL divergence)
3. Realized that since we don't know truth, we need a **relative** measure
4. Learned that $AIC$ is that relative measure
- how well can a model predict new data? 

$$AIC = D_{\text{train}} + 2p$$
  
